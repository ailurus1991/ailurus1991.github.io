<!doctype html><html lang="en" class="no-js"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="lang:clipboard.copy" content="Copy to clipboard"><meta name="lang:clipboard.copied" content="Copied to clipboard"><meta name="lang:search.language" content="en"><meta name="lang:search.pipeline.stopwords" content="True"><meta name="lang:search.pipeline.trimmer" content="True"><meta name="lang:search.result.none" content="No matching documents"><meta name="lang:search.result.one" content="1 matching document"><meta name="lang:search.result.other" content="# matching documents"><meta name="lang:search.tokenizer" content="[\s\-]+"><link rel="shortcut icon" href="../../assets/images/favicon.png"><meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.3.0"><title>信息论 - 啊又一个博客</title><link rel="stylesheet" href="../../assets/stylesheets/application.4031d38b.css"><link rel="stylesheet" href="../../assets/stylesheets/application-palette.224b79ff.css"><meta name="theme-color" content="#009688"><script src="../../assets/javascripts/modernizr.74668098.js"></script><link href="https://fonts.gstatic.com" rel="preconnect" crossorigin><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Serif+SC:300,400,400i,700|Ubuntu+Mono&display=swap"><style>body,input{font-family:"Noto Serif SC","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Ubuntu Mono","Courier New",Courier,monospace}</style><link rel="stylesheet" href="../../assets/fonts/material-icons.css"></head><body dir="ltr" data-md-color-primary="teal" data-md-color-accent="teal"><svg class="md-svg"><defs></defs></svg> <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off"> <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off"> <label class="md-overlay" data-md-component="overlay" for="__drawer"></label><a href="#_1" tabindex="1" class="md-skip">Skip to content </a><header class="md-header" data-md-component="header"><nav class="md-header-nav md-grid"><div class="md-flex"><div class="md-flex__cell md-flex__cell--shrink"><a href="../.." title="啊又一个博客" class="md-header-nav__button md-logo"><i class="md-icon">local_cafe</i></a></div><div class="md-flex__cell md-flex__cell--shrink"><label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label></div><div class="md-flex__cell md-flex__cell--stretch"><div class="md-flex__ellipsis md-header-nav__title" data-md-component="title"><span class="md-header-nav__topic">啊又一个博客</span><span class="md-header-nav__topic">信息论</span></div></div><div class="md-flex__cell md-flex__cell--shrink"><label class="md-icon md-icon--search md-header-nav__button" for="__search"></label><div class="md-search" data-md-component="search" role="dialog"><label class="md-search__overlay" for="__search"></label><div class="md-search__inner" role="search"><form class="md-search__form" name="search"><input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active"> <label class="md-icon md-search__icon" for="__search"></label> <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">&#xE5CD;</button></form><div class="md-search__output"><div class="md-search__scrollwrap" data-md-scrollfix><div class="md-search-result" data-md-component="result"><div class="md-search-result__meta">Type to start searching</div><ol class="md-search-result__list"></ol></div></div></div></div></div></div></div></nav></header><div class="md-container"><main class="md-main"><div class="md-main__inner md-grid" data-md-component="container"><div class="md-sidebar md-sidebar--primary" data-md-component="navigation"><div class="md-sidebar__scrollwrap"><div class="md-sidebar__inner"><nav class="md-nav md-nav--primary" data-md-level="0"><label class="md-nav__title md-nav__title--site" for="__drawer"><a href="../.." title="啊又一个博客" class="md-nav__button md-logo"><i class="md-icon">local_cafe</i></a>啊又一个博客</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../.." title="Home" class="md-nav__link">Home</a></li><li class="md-nav__item md-nav__item--active md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2" checked><label class="md-nav__link" for="nav-2">Basics</label><nav class="md-nav" data-md-component="collapsible" data-md-level="1"><label class="md-nav__title" for="nav-2">Basics</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item md-nav__item--active"><input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc"><label class="md-nav__link md-nav__link--active" for="__toc">信息论</label><a href="./" title="信息论" class="md-nav__link md-nav__link--active">信息论</a><nav class="md-nav md-nav--secondary"><label class="md-nav__title" for="__toc">Table of contents</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="#_2" title="自信息" class="md-nav__link">自信息</a><nav class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a href="#_3" title="信息熵" class="md-nav__link">信息熵</a><nav class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a href="#_4" title="联合信息熵" class="md-nav__link">联合信息熵</a></li></ul></nav></li></ul></nav></li><li class="md-nav__item"><a href="#_5" title="条件信息熵" class="md-nav__link">条件信息熵</a></li><li class="md-nav__item"><a href="#_6" title="互信息" class="md-nav__link">互信息</a></li><li class="md-nav__item"><a href="#kl-divergence" title="相对熵（KL Divergence）" class="md-nav__link">相对熵（KL Divergence）</a></li><li class="md-nav__item"><a href="#_7" title="交叉熵" class="md-nav__link">交叉熵</a></li><li class="md-nav__item"><a href="#_8" title="交叉熵和相对熵" class="md-nav__link">交叉熵和相对熵</a></li><li class="md-nav__item"><a href="#_9" title="相对熵补充" class="md-nav__link">相对熵补充</a></li><li class="md-nav__item"><a href="#js" title="JS散度" class="md-nav__link">JS散度</a></li><li class="md-nav__item"><a href="#_10" title="一些总结" class="md-nav__link">一些总结</a></li></ul></nav></li><li class="md-nav__item"><a href="../tfidf/" title="TF-IDF" class="md-nav__link">TF-IDF</a></li></ul></nav></li></ul></nav></div></div></div><div class="md-sidebar md-sidebar--secondary" data-md-component="toc"><div class="md-sidebar__scrollwrap"><div class="md-sidebar__inner"><nav class="md-nav md-nav--secondary"><label class="md-nav__title" for="__toc">Table of contents</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="#_2" title="自信息" class="md-nav__link">自信息</a><nav class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a href="#_3" title="信息熵" class="md-nav__link">信息熵</a><nav class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a href="#_4" title="联合信息熵" class="md-nav__link">联合信息熵</a></li></ul></nav></li></ul></nav></li><li class="md-nav__item"><a href="#_5" title="条件信息熵" class="md-nav__link">条件信息熵</a></li><li class="md-nav__item"><a href="#_6" title="互信息" class="md-nav__link">互信息</a></li><li class="md-nav__item"><a href="#kl-divergence" title="相对熵（KL Divergence）" class="md-nav__link">相对熵（KL Divergence）</a></li><li class="md-nav__item"><a href="#_7" title="交叉熵" class="md-nav__link">交叉熵</a></li><li class="md-nav__item"><a href="#_8" title="交叉熵和相对熵" class="md-nav__link">交叉熵和相对熵</a></li><li class="md-nav__item"><a href="#_9" title="相对熵补充" class="md-nav__link">相对熵补充</a></li><li class="md-nav__item"><a href="#js" title="JS散度" class="md-nav__link">JS散度</a></li><li class="md-nav__item"><a href="#_10" title="一些总结" class="md-nav__link">一些总结</a></li></ul></nav></div></div></div><div class="md-content"><article class="md-content__inner md-typeset"><h1 id="_1"><a class="toclink" href="#_1">信息论复习</a><a class="headerlink" href="#_1" title="Permanent link">&para;</a></h1>
<h2 id="_2"><a class="toclink" href="#_2">自信息</a><a class="headerlink" href="#_2" title="Permanent link">&para;</a></h2>
<p>是对一个事件本身所拥有的信息量多少的衡量，一个事件如果自信息值比较大，那么可以说它所包含的信息量比较大。</p>
<p>假设信息量用<span><span class="MathJax_Preview">I</span><script type="math/tex">I</script></span>来表示，对于一个事件A来说，那么它含有的信息量就是<span><span class="MathJax_Preview">I(A)</span><script type="math/tex">I(A)</script></span>.</p>
<p>有一些定义：</p>
<ol>
<li>
<p>信息量的大小显然和这个事件发生的概率是有关系的，假如一个事件的发生的概率是100%，那么这个事件显然所含有的信息量就是0。
    &gt; 告诉我明天太阳会从东边升起，那么显然这句话从信息量的角度来说就是一句废话，它发生的概率是100%，它含有的信息量是0，自信息也是0。</p>
</li>
<li>
<p>假设有两个独立的事件A和B，C是它们的交集，那么分别宣告A和B的信息量等于宣告C的信息量。也就是<span><span class="MathJax_Preview">I(C)=I(A)+I(B)</span><script type="math/tex">I(C)=I(A)+I(B)</script></span>
    &gt; 告诉我苹果很好吃以及它是红色的这两句话等于先告诉我它很好吃，然后再告诉我它是红色的。</p>
</li>
</ol>
<p>从1来看，既然信息量和概率有关系，那我们不妨假设一个函数<span><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span>，它的输入是事件发生的概率，例如<span><span class="MathJax_Preview">f(p(w))</span><script type="math/tex">f(p(w))</script></span>，输出就是信息量，也就是<span><span class="MathJax_Preview">I(w)=f(p(w))</span><script type="math/tex">I(w)=f(p(w))</script></span>。</p>
<p>那么从2来看，自然就有了：<span><span class="MathJax_Preview">f(p(w_c))=f(p(w_a))+f(p(w_b))</span><script type="math/tex">f(p(w_c))=f(p(w_a))+f(p(w_b))</script></span>。以及我们还知道C是A和B两个独立事件的交集，所以还有<span><span class="MathJax_Preview">p(w_c)=p(w_a) \cdot p(w_b)</span><script type="math/tex">p(w_c)=p(w_a) \cdot p(w_b)</script></span>，把这个带入前面的公式，就可以得到：</p>
<div>
<div class="MathJax_Preview">f(p(w_a)\cdot p(w_b)) = f(p(w_a)) + f(p(w_b))</div>
<script type="math/tex; mode=display">f(p(w_a)\cdot p(w_b)) = f(p(w_a)) + f(p(w_b))</script>
</div>
<p>这个看起来就是<span><span class="MathJax_Preview">f(x\cdot y)=f(x)+f(y)</span><script type="math/tex">f(x\cdot y)=f(x)+f(y)</script></span>，于是可以天然地使用对数函数来表示<span><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span>，所以对于一个事件A来说，它的自信息一般表示就是<span><span class="MathJax_Preview">Klog(p(A))</span><script type="math/tex">Klog(p(A))</script></span>，<span><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span>是一个常数。由于<span><span class="MathJax_Preview">p(A)</span><script type="math/tex">p(A)</script></span> 肯定是一个0～1的数字，所以为了让自信息是正数，一般<span><span class="MathJax_Preview">K&lt;0</span><script type="math/tex">K<0</script></span>，最常见的就是使用<span><span class="MathJax_Preview">-log(p(w))</span><script type="math/tex">-log(p(w))</script></span>。</p>
<p>还有一个地方需要提一下，这里的对数底是未知的，一般来说，如果2为底数，这个计算的单位就是bit，如果是e为底数，单位是nat，10为底数，单位是hart。</p>
<h3 id="_3"><a class="toclink" href="#_3">信息熵</a><a class="headerlink" href="#_3" title="Permanent link">&para;</a></h3>
<p>对于一个变量<span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>来说，可能会有多个不同的取值，假设神经网络最后一层只有一个神经元，我们需要的分类是两类，那么我们可以以0.5为分界线，最近的取值是0或者1。那么显然这种情况下，0是一个事件，1也是一个事件。这两个事件是独立的，那么对应多个事件的<span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>的信息熵就是：<span><span class="MathJax_Preview">H(x)=E[I(x)]</span><script type="math/tex">H(x)=E[I(x)]</script></span>，换句话来说，变量的信息熵，等于变量中事件的自信息量的期望。</p>
<p>那么对于前面的含有两个事件的变量来说，它的信息熵就是：</p>
<div>
<div class="MathJax_Preview">H(x)=(-p(x=0) log(p(x=0)) ) + (-p(x=1)log(p(x=1)))</div>
<script type="math/tex; mode=display">H(x)=(-p(x=0) log(p(x=0)) ) + (-p(x=1)log(p(x=1)))</script>
</div>
<p>简单来说，就是各自的概率乘以各自的自信息量，对于这种两个事件的情况，化简一下就是：<span><span class="MathJax_Preview">H(x)=-plog(p)-(1-p)log(1-p)</span><script type="math/tex">H(x)=-plog(p)-(1-p)log(1-p)</script></span></p>
<p>求导数可以知道<span><span class="MathJax_Preview">p=0.5</span><script type="math/tex">p=0.5</script></span>的时候熵是最大的，如果有多个事件，那么当它们的概率是相等的时候，熵是最大的。</p>
<blockquote>
<p>简而言之，0.5的概率表示我们对于这个变量一无所知，那么信息的不确定性就最大。</p>
</blockquote>
<h4 id="_4"><a class="toclink" href="#_4">联合信息熵</a><a class="headerlink" href="#_4" title="Permanent link">&para;</a></h4>
<p>假设有两个不同的变量<span><span class="MathJax_Preview">x,y</span><script type="math/tex">x,y</script></span>，它们各自都会有不同的取值，例如<span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>可能是0或者1，<span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>可能是字符串“a”或者“b”，那么每一组取值就是一个事件，联合信息熵就是所有事件自信息量的期望：</p>
<div>
<div class="MathJax_Preview">H(x,y)=-\sum_x \sum_y p(x,y)log(p(x,y))</div>
<script type="math/tex; mode=display">H(x,y)=-\sum_x \sum_y p(x,y)log(p(x,y))</script>
</div>
<p>联合信息熵是用来评价联合分布的不确定性。</p>
<p>对<span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> 和 <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> 可以有一些假设，假如它们是完全无关的，例如<span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>表示的是北京明天会不会下雨，<span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>表示的是我的程序会不会正常工作。那么显然<span><span class="MathJax_Preview">H(x,y)=H(x)+H(y)</span><script type="math/tex">H(x,y)=H(x)+H(y)</script></span>，因为它们是完全无关的，再反过来说，假如<span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>和<span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>是完全相关，也就是每一个<span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>都能对应到一个<span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>，例如<span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>表示的是明天北京会不会下雨，<span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>表示的是明天路上会不会有雨水。因为它们是完全相关，所以<span><span class="MathJax_Preview">H(x,y)=H(x)=H(y)</span><script type="math/tex">H(x,y)=H(x)=H(y)</script></span>。</p>
<p>除此之外的大部分情况都有：<span><span class="MathJax_Preview">H(x,y) \leqslant H(x)+H(y)</span><script type="math/tex">H(x,y) \leqslant H(x)+H(y)</script></span>, <span><span class="MathJax_Preview">H(x,y) \geqslant  H(x)</span><script type="math/tex">H(x,y) \geqslant  H(x)</script></span>, <span><span class="MathJax_Preview">H(x,y) \geqslant H(y)</span><script type="math/tex">H(x,y) \geqslant H(y)</script></span></p>
<h2 id="_5"><a class="toclink" href="#_5">条件信息熵</a><a class="headerlink" href="#_5" title="Permanent link">&para;</a></h2>
<p>是指在给定其中一个变量<span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>的情况下，事件<span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>发生的不确定性：</p>
<div>
<div class="MathJax_Preview">H(y|x) = -\sum_x \sum_y p(x,y) log(p(y|x)) </div>
<script type="math/tex; mode=display">H(y|x) = -\sum_x \sum_y p(x,y) log(p(y|x)) </script>
</div>
<p>依据概率公式：</p>
<div>
<div class="MathJax_Preview">H(y|x) = -\sum_x \sum_y p(x,y) log(\frac{p(x,y)}{p(x)}) = -=\sum_x \sum_y p(x,y)(log(p(x,y)) - log(p(x)))</div>
<script type="math/tex; mode=display">H(y|x) = -\sum_x \sum_y p(x,y) log(\frac{p(x,y)}{p(x)}) = -=\sum_x \sum_y p(x,y)(log(p(x,y)) - log(p(x)))</script>
</div>
<p>上面的其实就是：<span><span class="MathJax_Preview">H(x,y)-H(x)</span><script type="math/tex">H(x,y)-H(x)</script></span>，条件信息熵就是联合信息熵-先验变量的熵。</p>
<h2 id="_6"><a class="toclink" href="#_6">互信息</a><a class="headerlink" href="#_6" title="Permanent link">&para;</a></h2>
<p>互信息的定义比较特殊，表示的是有两个变量，那么在确定了一个变量的情况下，另一个变量的条件熵相比不确定的情况下，值减少的量。定义是<span><span class="MathJax_Preview">I(x;y)=H(y)-H(y|x)</span><script type="math/tex">I(x;y)=H(y)-H(y|x)</script></span>。</p>
<p>这个值的大小和联合信息熵之间有一些关系。假设<span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>和<span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>之间关系明显，是一一对应的，那么显然<span><span class="MathJax_Preview">H(y|x)</span><script type="math/tex">H(y|x)</script></span>就是0，因为知道了<span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>，<span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>的不确定性变成0了，它不具备任何信息。那么<span><span class="MathJax_Preview">I(x;y)=H(y)</span><script type="math/tex">I(x;y)=H(y)</script></span>，就会比较大，也就是<span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>和<span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>之间的互信息非常大。</p>
<p>反过来如果<span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>和<span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>之间没有关系，那么<span><span class="MathJax_Preview">H(y|x)</span><script type="math/tex">H(y|x)</script></span>就和<span><span class="MathJax_Preview">H(y)</span><script type="math/tex">H(y)</script></span>相等，没有任何贡献，知道不知道<span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>无所谓，那么此时的互信息就是0。</p>
<p>上面几个量之间的关系：
<img alt="" src="http://ww1.sinaimg.cn/large/006zmP1Wly1g37ugddw0qj306w04cmx8.jpg" /></p>
<h2 id="kl-divergence"><a class="toclink" href="#kl-divergence">相对熵（KL Divergence）</a><a class="headerlink" href="#kl-divergence" title="Permanent link">&para;</a></h2>
<p>上面的信息熵其实是一个随机变量的几个事件带来的信息量的大小，或者是条件信息熵那种多个不同的变量之间的关系。</p>
<p>但是交叉熵是用来衡量一个变量，但是这个变量其实是存在多个不同的分布，需要去衡量两个分布之间的差异性。</p>
<p>假设这个变量是<span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>，<span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>存在的两个分布分别是<span><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span>和<span><span class="MathJax_Preview">q(x)</span><script type="math/tex">q(x)</script></span>。</p>
<p>KL Divergence的定义是：</p>
<div>
<div class="MathJax_Preview">D_{KL}(p||q)=\sum_i p(x_i)log(\frac{p(x_i)}{q(x_i )})</div>
<script type="math/tex; mode=display">D_{KL}(p||q)=\sum_i p(x_i)log(\frac{p(x_i)}{q(x_i )})</script>
</div>
<p>有几个地方需要注意</p>
<ul>
<li><span><span class="MathJax_Preview">log</span><script type="math/tex">log</script></span>里面是用两个分布的比值来衡量，这是比较合理的，显然当两个分布是一样的时候，整个KL Divergence就是0了。</li>
<li>但是这个比值外面的期望权重是<span><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span>的，这一点需要注意。也就是表示KL Divergence的前面那个东西。</li>
</ul>
<p>注意到这个期望只是<span><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span>的概率，那么既然KL Divergence是表述两者之间的距离，那么<span><span class="MathJax_Preview">q</span><script type="math/tex">q</script></span>为什么不见了。</p>
<p>其实对应的还有一个：</p>
<div>
<div class="MathJax_Preview">D_{KL}(q||p)=\sum_i p(x_i)log(\frac{q(x_i)}{ p(x_i )})</div>
<script type="math/tex; mode=display">D_{KL}(q||p)=\sum_i p(x_i)log(\frac{q(x_i)}{ p(x_i )})</script>
</div>
<p>所以KL散度显然不具备对称性，p到q的距离不等于q到p的距离。因为这里散度的定义其实还有一个概念，就是**确认方向**和**辨识主体**。</p>
<h2 id="_7"><a class="toclink" href="#_7">交叉熵</a><a class="headerlink" href="#_7" title="Permanent link">&para;</a></h2>
<p>继续说交叉熵，他同样是来表示两个分布之间的距离，这个角度上和相对熵的功能是一样的。</p>
<p>这里可以定义<span><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span>是真实的分布，可以认为是target的分布，<span><span class="MathJax_Preview">q(x)</span><script type="math/tex">q(x)</script></span>是模型的预测的分布。</p>
<p>前面说过对于p的信息熵：</p>
<div>
<div class="MathJax_Preview">H(p) =\sum_x p(x) log(\frac{1}{p(x)})</div>
<script type="math/tex; mode=display">H(p) =\sum_x p(x) log(\frac{1}{p(x)})</script>
</div>
<p>信息熵的计算的意义一方面是代表p本身所含有的信息量，也是用来表示如果需要表述p的分布，需要多少的存储空间。也就是识别一个样本需要的平均编码长度。</p>
<p>那么如果用一个假的，错误的分布来表示真实分布的平均编码长度呢？</p>
<div>
<div class="MathJax_Preview">H_{cross}(p,q) = \sum_x p(x) log(\frac{1}{q(x)})</div>
<script type="math/tex; mode=display">H_{cross}(p,q) = \sum_x p(x) log(\frac{1}{q(x)})</script>
</div>
<p>这里的<span><span class="MathJax_Preview">q</span><script type="math/tex">q</script></span>其实就是一个假的，错误的分布。那么用一个假的分布来表示真实分布的编码的意义是什么呢？换句话说，为什么要用一个假的分布来做这件事。</p>
<p>不妨从编码信息量的角度来解释：</p>
<blockquote>
<p>比如含有4个字母(A,B,C,D)的数据集中，真实分布p=(&frac12;, &frac12;, 0, 0)，即A和B出现的概率均为&frac12;，C和D出现的概率都为0。计算H(p)为1，即只需要1位编码即可识别A和B。如果使用分布Q=(&frac14;, &frac14;, &frac14;, &frac14;)来编码则得到H(p,q)=2，即需要2位编码来识别A和B(当然还有C和D，尽管C和D并不会出现，因为真实分布p中C和D出现的概率为0，这里就钦定概率为0的事件不会发生啦)。</p>
</blockquote>
<p>上面这个知乎的例子比较容易理解，在真实分布自己的信息熵表示时候，其实表示的是表达自己需要的最少的存储空间，那么当使用错误的分布来进行表示的时候，如果还想把原来的所有可能的信息表达进去，那么就需要更多的存储空间。例如上面的例子中，原来的分布表达的时候就只需要对A和B进行编码就可以了，因为我们已经知道C和D发生的概率是0。但是当使用一个错误的分布的时候，就还需要编码多余的一些C和D进去。这些C和D所占的存储空间从结果的角度来看就是被浪费掉的。</p>
<p>所以，交叉熵就是用来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出成本的大小。</p>
<p>注意这里面交叉熵也是具备方向性的，也不对称。前面的公式中，p是真实分布，q是虚假的分布。</p>
<p>所以其实说起来，交叉熵并不是直接用来衡量两个分布之间的差异大小，但是换过来说，如果两个分布之间差异很小，那么自然需要的额外成本就很小，极端情况就是真实分布来表述自己，那显然就等于分布自己信息熵。</p>
<h2 id="_8"><a class="toclink" href="#_8">交叉熵和相对熵</a><a class="headerlink" href="#_8" title="Permanent link">&para;</a></h2>
<p>前面面提到的相对熵的公式可以进行化简计算：</p>
<div>
<div class="MathJax_Preview">D_{KL}(p||q)=\sum_i p(x_i)log(\frac{p(x_i)}{q(x_i )})=H_{cross}(p,q)-H(p)</div>
<script type="math/tex; mode=display">D_{KL}(p||q)=\sum_i p(x_i)log(\frac{p(x_i)}{q(x_i )})=H_{cross}(p,q)-H(p)</script>
</div>
<p>从这里看的话就非常明显了，KL散度实际上就是如果使用虚假的的分布q来编码原来的真实分布，所需要的额外的存储空间。</p>
<p>所以其实按理说，我们在机器学习中，如果要去衡量target和模型输出之间的差异，最好的做法是使用KL散度。</p>
<p>但是这里由于公式中可以看出来，其实交叉熵和相对熵之间差的就是一个真实分布p的信息熵，这个值其实就是target的信息熵，这个值本来也就是一个常量，所以正常的loss用交叉熵也可以达到一样的效果。</p>
<h2 id="_9"><a class="toclink" href="#_9">相对熵补充</a><a class="headerlink" href="#_9" title="Permanent link">&para;</a></h2>
<p>前面提到，如果想衡量两个分布之间的差异，那么相对熵是最正确，最直接的选择。那么前面也提到了相对熵其实是有方向性的，那么究竟如何来选择方向？<span><span class="MathJax_Preview">D_{KL}(p||q)</span><script type="math/tex">D_{KL}(p||q)</script></span>和<span><span class="MathJax_Preview">D_{KL}(q||p)</span><script type="math/tex">D_{KL}(q||p)</script></span>之间有什么区别？</p>
<p>先说散度求解的目的，其实是找到一个q来模拟真实分布p，让KL散度值最小，于是变成一个优化问题。</p>
<p>先说正向KL散度，也就是前面定义的：</p>
<div>
<div class="MathJax_Preview">q* = \arg  \min_{q} D_{KL}(p||q) = \arg \min_q \sum_x p(x) log(\frac{p(x)}{q(x)})</div>
<script type="math/tex; mode=display">q* = \arg  \min_{q} D_{KL}(p||q) = \arg \min_q \sum_x p(x) log(\frac{p(x)}{q(x)})</script>
</div>
<p>这样的优化方程会带来什么样的结果呢？</p>
<p>显然p是已知的，所以我们希望p大的时候，q也大，p小的时候，就无所谓了，因为那一项本来就小，所以这种优化函数的结果就是q在p比较大的时候，和p比较接近，p比较小的时候，无所谓。</p>
<p>再来看看反向KL散度，定义是反过来的：</p>
<div>
<div class="MathJax_Preview">q* = \arg \min_q D_{KL}(q||p) = \arg \min_q \sum_x q(x)log(\frac{q(x)}{p(x)})</div>
<script type="math/tex; mode=display">q* = \arg \min_q D_{KL}(q||p) = \arg \min_q \sum_x q(x)log(\frac{q(x)}{p(x)})</script>
</div>
<p>这里q首先是未知的，在已知p的情况下，如果p比较小，那么我们希望q也跟着小，如果p比较大，那么其实不是太有所谓。</p>
<p>所以其实可以看出来，如果使用正向的KL散度，那么得到的q分布，其实更加关注真实分布中的大概率事件，反过来如果使用反向KL散度，那么得到的q分布，就更加关注真实分布中的罕见事件。</p>
<p>补充的一个例子，如果p是两个高斯分布的叠加，有两个峰值，如果q设定是一个单峰的高斯分布，那么求出来的q会是什么样子呢？</p>
<p><img alt="" src="http://ww1.sinaimg.cn/large/006zmP1Wly1g37yyxhzrcj30zw0hmjww.jpg" /></p>
<p>选择使用KL散度的哪个方向是取决于实际问题的。它们并没有熟好熟劣的区分。一些应用需要这个近似分布q在真实分布p放置高概率的所有地方都放置高概率，而其他应用需要这个近似分布q在真实分布p放置低概率的所有地方都很少放置高概率。KL散度方向的选择反映了对于每种应用，优先考虑哪一种选择。</p>
<p>上面左图是最小化<span><span class="MathJax_Preview">D_{KL}(p||q)</span><script type="math/tex">D_{KL}(p||q)</script></span>的效果。在这种情况下，我们选择一个q使得它在p具有高概率的地方具有高概率，意味着在优化过程中，更在意真实分布p中的常见事件，也就是蓝线的两峰，我们要优先确保它们在分布q中不是特别罕见（信息长度不是特别长）。当p具有多个峰时，q选择将这些峰模糊到一起，以便将高概率质量放到所有峰上。</p>
<p>上面右图是最小化<span><span class="MathJax_Preview">D_{KL}(q||p)</span><script type="math/tex">D_{KL}(q||p)</script></span> 的效果。在这种情况下，我们选择一个q使得它在p具有低概率的地方具有低概率，意味着在优化过程中，更在意真实分布p中的罕见事件，也就是蓝线的谷底，我们要优先确保它们在分布q中不是特别常见（信息长度特别长的那些事件）。当p具有多个峰并且这些峰间隔很宽时，如该图所示，最小化KL散度会选择单个峰，以避免将概率质量放置在p的多个峰之间的低概率区域中。图中是当q被选择成强调左边峰时的结果，我们也可以通过选择右边峰来得到KL散度相同的值。如果这些峰没有被足够强的低概率区域分离，那么KL散度的这个方向仍然可能选择模糊这些峰。</p>
<p>在二维特征情况下，是这样：</p>
<p><img alt="" src="http://ww1.sinaimg.cn/large/006zmP1Wly1g37z0esb6rj313i0digsd.jpg" /></p>
<p>左边，q趋近于完全覆盖p。
中、右图：q能够锁定某一个峰值。</p>
<h2 id="js"><a class="toclink" href="#js">JS散度</a><a class="headerlink" href="#js" title="Permanent link">&para;</a></h2>
<p>由于KL散度的不对称，所以还有一个JS散度，其实是对KL散度做了一个对称操作：</p>
<div>
<div class="MathJax_Preview">D_{JS}(p||q)=\frac{1}{2}D_{KL}(p||\frac{p+q}{2})+\frac{1}{2}D_{KL}(q||\frac{p+q}{2})</div>
<script type="math/tex; mode=display">D_{JS}(p||q)=\frac{1}{2}D_{KL}(p||\frac{p+q}{2})+\frac{1}{2}D_{KL}(q||\frac{p+q}{2})</script>
</div>
<h2 id="_10"><a class="toclink" href="#_10">一些总结</a><a class="headerlink" href="#_10" title="Permanent link">&para;</a></h2>
<p>所以怎么说呢，像ML中使用默认的交叉熵，其实就是说明我们更关注神经元那些被激活的点，概率大的地方，而不是罕见事件。</p>
<p>ref: <a href="https://lumingdong.cn/various-entropies-in-machine-learning.html">https://lumingdong.cn/various-entropies-in-machine-learning.html</a></p></article></div></div></main><footer class="md-footer"><div class="md-footer-nav"><nav class="md-footer-nav__inner md-grid"><a href="../.." title="Home" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev"><div class="md-flex__cell md-flex__cell--shrink"><i class="md-icon md-icon--arrow-back md-footer-nav__button"></i></div><div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title"><span class="md-flex__ellipsis"><span class="md-footer-nav__direction">Previous</span>Home</span></div></a><a href="../tfidf/" title="TF-IDF" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next"><div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title"><span class="md-flex__ellipsis"><span class="md-footer-nav__direction">Next</span>TF-IDF</span></div><div class="md-flex__cell md-flex__cell--shrink"><i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i></div></a></nav></div><div class="md-footer-meta md-typeset"><div class="md-footer-meta__inner md-grid"><div class="md-footer-copyright">powered by <a href="https://www.mkdocs.org">MkDocs</a> and <a href="https://squidfunk.github.io/mkdocs-material/">Material for MkDocs</a></div><div class="md-footer-social"><link rel="stylesheet" href="../../assets/fonts/font-awesome.css"> <a href="https://twitter.com/ailurus1991" class="md-footer-social__link fa fa-twitter"></a> </div></div></div></footer></div><script src="../../assets/javascripts/application.b260a35d.js"></script><script>app.initialize({version:"1.0.4",url:{base:"../.."}})</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>