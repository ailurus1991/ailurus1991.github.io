为什么TF-IDF在关键词抽取这样的应用中是一个比较简单有效的baseline呢，尝试从信息论的角度对TF-IDF做一些解释。

常见的使用背景是有一个语料库，语料库中有1000篇文章，现在希望给每篇文章找几个关键词。

TF-IDF需要计算两个值，TF value和IDF value，整个TF-IDF其实是TF value和IDF value的乘积。

先说说TF value。TF是 Term Frequency，也就是词频统计。计算TF value的时候需要指定某个词，以及某一篇文章，然后直接统计这个词在这篇文章中出现的频率。

> 我爱吃苹果，苹果爱吃梨

这篇文章中 **苹果** 的词频就是2，**我** 的词频就是1。

最简单的TF value就可以直接使用TF的统计值，在上面的例子中，对于 **苹果** 和 **我** 这两个词，可以直接使用2和1。使用TF来衡量某个词对这句话的重要性，简单直接，毕竟词频越高，它在这篇文章中自然也就越重要。不过大部分时候并不会直接用，会做一些平滑或者异常处理，比如：

* 0/1：如果rawTF>0，取1，或者取0。
* 简单的norm：rawTF/句子所有的词的数量
* 加上log做平滑：log(1+rawTF)

再说说IDF value，IDF 是 Inverse Document Frequency，字面翻译是逆文档频率。逆文档频率计算稍稍复杂，计算 IDF 需要指定某个词，以及统计整个语料库才行。

假设整个语料库有1000篇文章，其中有10篇文章中包含了 **苹果** 这个词。那么 **苹果** 这个词的IDF就是：

$$IDF=log(总文章数/包含苹果的文章数)=log(1000/10)$$

可以看出，$总文章数/包含苹果的文章数$ 这个值一定大于等于1。对某个词来说，如果语料库中包含这个词的文档越多，那么log里面的值就会从非常慢慢接近1，整个log的值也就从很大慢慢接近0。直观的理解就是既然语料库中，大部分文档都包含这个词，甚至所有文章都有这个词，那么显然这个词就不重要，没有任何识别性。反过来，如果这个词只出现一次，语料库越大，那么log值也就会越大。

简而言之，一个词的 IDF value 代表的是它在整个语料库中的稀有程度。

将TF value和IDF value乘起来就是最后的TF-IDF value，那么什么样的词会有比较大的TF-IDF value呢，对某篇文章来说，如果这个词在当前文章中出现频率高，但是在整个语料库中却比较稀有，那么它就会成为这篇文章的关键词。

举一个例子好了。例如小李同学在火车站被坏人抢走了手机，警察询问小李：“你有没有观察到坏人有什么样的特征”。小李可以回答：坏人是男性，短发。或者是小李想了想回答：坏人肩膀上有紫色的鸟类羽毛纹身。这两种回答对警察来说，具备的意义是完全不同的。纹身特征相比男性，短发来说，显然在整个嫌疑人库中出现的频率更低，这样的特征应当成为嫌疑人的关键特征。所以换句话来说， 如果给嫌疑人提取关键词，那么他应该被标记上“纹身”而不是“男性”，“短发”。

TF-IDF如此直观有效的计算方式其实可以从信息论的角度来解释一下：

假如现在指定一个事件，从语料库中找一篇文章，它恰好包含 **苹果**，我们将这个事件发生的概率定做$p$。

那么p的信息熵其实是$I(p) = -log(p)$，如果包含 **苹果** 这个词的文章的数量是$D_i$，以及整个语料库的文章的数量是$D$，那么显然有$p=\frac{D_i}{D}$，再把p放回信息熵的公式，就可以得到：

$$I(p)=log(\frac{D}{D_i})$$

所以 **某个词的IDF值** 其实就是：语料库中随机找一篇文章，文章包含这个词的事件的信息熵。可以想象，如果某个词很常见，语料库中随便找一篇文章，基本都有它。那么当我告诉你，刚刚随机找的这篇文章中发现这个词，这件事的信息熵很低。

再说一些设定：

* 当前词语是$i$
* 当前文章的词的个数是$m$
* 当前文章中，词语$i$出现的次数是$n_i$
* 语料库的总文档数量是$D$
* 语料库中包含当前词语$i$的文档的数量是$D_i$

所以对于某个词$i$来说，它对应的TF-IDF计算就是：

$$\frac{n_i}{m}log(\frac{D}{D_i})$$

下面我们对这个TF-IDF计算公式进行一些变形，数学上它等于下面：

$$\frac{n_i}{m}log(\frac{ \frac{n_i}{m}}{\frac{n_i D_i}{m D}})$$

这么做有什么意义呢，想要让它有一些意义，需要增加一些假设：

* 刚才说：当前文章的词的个数是$m$，现在我们假设所有文章的词的个数都是$m$，所有文章都一样长。
* 前面设定了对于当前文章，词语$i$出现的次数是$n_i$，现在我们增加一个假设，也就是语料库中所有文章，只要出现了词语$i$，出现的次数都是一样的，全都是$n_i$。举例来说，如果语料库有100篇文章，其中10篇提到了 **苹果** 这个词，那么这10篇文章中出现 **苹果** 这个词的次数是一样的，比如都是1次，或者都是3次。

有了这两个假设，继续看看上面的公式：

$\frac{n_i}{m}$就变成了从文章中随便挑一个词出来，这个词恰好是$i$的概率。

那么，$\frac{n_i D_i}{m D}$ 是什么呢，分子$n_i D_i$表示的是整个语料库中，$i$这个词出现的次数，因为$D_i$ 是包含$i$的总的文章数量，上面假设又有次数一样，所以它们相乘就是总次数。接着说分母，$m D$其实就是整个语料库的词的数量，因为已经假设了所有文章含有的词的数量都是一样的。那么整个分数代表的物理意义就是：从语料库中随便找一个词，它是$i$的概率。

再继续说回前面的变形过的公式的IDF部分：

$$log(\frac{ \frac{n_i}{m}}{\frac{n_i D_i}{m D}})$$

这其实代表的是衡量当前文章中词语$i$的出现概率和整个语料库中词语$i$出现的概率的差异比值。

完整的变形后的公式是：

$$\frac{n_i}{m}log(\frac{ \frac{n_i}{m}}{\frac{n_i D_i}{m D}})$$

它长得非常像相对熵的公式：

$$D_{KL}(p||q)=\sum_i p(x_i)log(\frac{p(x_i)}{q(x_i )})$$

所以到这一步其实就真相大白了：

某个词的TF-IDF就是这个词在当前文档的出现的分布和它在整个语料库中出现的分布的相对熵。当然其实需要前面提到的许多假设条件。

那么说回来，相对熵的物理意义其实是衡量两个分布之间的差异性，所以TF-IDF衡量的就是某个词在当前文章和整个语料库中的分布的差异性，差异性越大，就越能体现出这个词在这篇文章中的特殊性，内容指向性越强。

TF-IDF的初衷也正是在此，衡量词语对文章指向性的参考价值。

那么这个算法有什么问题呢，我们不妨从TF-IDF刚才往相对熵上靠拢时候提到的假设说起。假设中提到文档的长度差不多一致，以及出现次数也一致。所以如果TF-IDF的时候，如果语料库的文章长短差异性非常大，那么显然就不合适，以及出现的次数差异也很大，可能也不那么合适。

这两个假设其实是相辅相成的，在一个比较理想的场景，例如会议论文提取关键词，大家论文长度也都差不多，都是比较专业的作者，写法也不会差异太大（狂对切关键词），那么这个场景使用TF-IDF可能就是一个比较有效的办法。如果是长短不一，可能直接套用TF-IDF就会有问题。




